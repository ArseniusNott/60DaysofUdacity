trusted aggregator

Additive secret sharing
- aggregate gradients in an encrypted state
- necessary if we have no trusted aggregator
- allow multiple individuals to add numbers together without any person learning anyone else's inputs to the addition

Fixed Precision Encoding
- for negative and floating point numbers

pointerchain operators
wrapper> fixedprecisiontensor> tensor

when selecting a field size you are also setting the maximum size of the number that you can represent.

Now I realized that Secure and Private AI is not only about differential privacy. It is a collection of techniques that have strengths and weaknesses depending on the goals of privacy, trusts configuration, ease of implementation and the actors who will participate in achieving privacy-preserving data pipelines. Following the summary of Andrew Trask, I listed down these techniques and how they blend in to the aforementioned trade-offs:
1. PATE - useful when we want to label our datasets by utilizing private datasets or datasets that are sensitive in nature, effectively aggregating each output by some aggregation techniques (majority vote as the most common).
2. Epsilon-Delta Tool - useful when we want to constrain the access to our sensitive data or the other way around by assigning parameters epsilon and delta, adhering to the formal definition of privacy. These parameters determine how much data owners must trust data modelers/researchers to protect their privacy in the process of learning. 
3. Vanilla Federated Learning - useful when we don't want to aggregate collective training data for legal, social and logistic reasons so one must setup multiple actors that will share modeling process, then aggregate model information to a trusted aggregator. This requires a little bit of trust especially to the aggregator to not leak model information across actors.
4. Secure additive aggregation - useful when you solve privacy issues in federated learning by doing field additive aggregation techniques to encrypts model information so trusted aggregators cannot glean on raw gradients sent by federated nodes. 